{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dff2015",
   "metadata": {},
   "source": [
    "# 1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e45a2a",
   "metadata": {},
   "source": [
    "One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdacaa3",
   "metadata": {},
   "source": [
    "# 2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136afe4",
   "metadata": {},
   "source": [
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words. A measure of the presence of known words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd0386",
   "metadata": {},
   "source": [
    "# 3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13d046",
   "metadata": {},
   "source": [
    "A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dba911",
   "metadata": {},
   "source": [
    "# 4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801fc7f",
   "metadata": {},
   "source": [
    "TF-IDF is a popular approach used to weigh terms for NLP tasks because it assigns a value to a term according to its importance in a document scaled by its importance across all documents in your corpus, which mathematically eliminates naturally occurring words in the English language, and selects words that are more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742a95f",
   "metadata": {},
   "source": [
    "# 5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42f2de",
   "metadata": {},
   "source": [
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d7f40",
   "metadata": {},
   "source": [
    "# 6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d721e",
   "metadata": {},
   "source": [
    "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4475c8",
   "metadata": {},
   "source": [
    "# 7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a061e",
   "metadata": {},
   "source": [
    "Both are architectures to learn the underlying word representations for each word by using neural networks. ... In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc332fdd",
   "metadata": {},
   "source": [
    "# 8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d3aa5",
   "metadata": {},
   "source": [
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbeda49",
   "metadata": {},
   "source": [
    "# 9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7c0fd",
   "metadata": {},
   "source": [
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
