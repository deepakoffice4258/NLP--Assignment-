{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289af43b",
   "metadata": {},
   "source": [
    "# 1. Explain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404ee75",
   "metadata": {},
   "source": [
    "According to Tensorflow documentation, “An RNN cell, in the most abstract setting, is anything that has a state and performs some operation that takes a matrix of inputs.” RNN cells distinguish themselves from the regular neurons in the sense that they have a state and thus can remember information from the past"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c9018",
   "metadata": {},
   "source": [
    "# 2. Explain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f5150",
   "metadata": {},
   "source": [
    "BPTT works by unrolling all input timesteps. ... Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49203687",
   "metadata": {},
   "source": [
    "# 3. Explain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0365d",
   "metadata": {},
   "source": [
    "Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients. This problem happens because of weights, not because of the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9adf3",
   "metadata": {},
   "source": [
    "# 4. Explain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d00b75",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. ... A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95bb409",
   "metadata": {},
   "source": [
    "# 5. Explain Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c69fec",
   "metadata": {},
   "source": [
    "GRU or Gated recurrent unit is an advancement of the standard RNN i.e recurrent neural network. ... Just like LSTM, GRU uses gates to control the flow of information. They are relatively new as compared to LSTM. This is the reason they offer some improvement over LSTM and have simpler architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781f117",
   "metadata": {},
   "source": [
    "# 6. Explain Peephole LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37a79d",
   "metadata": {},
   "source": [
    "Peephole connections refer to a modification to the basic LSTM architecture. Surprisingly, LSTM augmented by “peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c049248",
   "metadata": {},
   "source": [
    "# 7. Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f1d12",
   "metadata": {},
   "source": [
    "Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0212b34",
   "metadata": {},
   "source": [
    "# 8. Explain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b99473",
   "metadata": {},
   "source": [
    "LSTMs use a gating mechanism that controls the memoizing process. Information in LSTMs can be stored, written, or read via gates that open and close. These gates store the memory in the analog format, implementing element-wise multiplication by sigmoid ranges between 0-1. ... Let's look at the architecture of an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c546e0",
   "metadata": {},
   "source": [
    "# 9. Explain BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a00278",
   "metadata": {},
   "source": [
    "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3092067",
   "metadata": {},
   "source": [
    "# 10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb3d59",
   "metadata": {},
   "source": [
    "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
