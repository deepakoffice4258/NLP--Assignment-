{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2188221",
   "metadata": {},
   "source": [
    "# 1. What are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de26c0",
   "metadata": {},
   "source": [
    "A typical sequence to sequence model has two parts – an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. ... This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce9d92",
   "metadata": {},
   "source": [
    "# 2. What are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58589eea",
   "metadata": {},
   "source": [
    "However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f57341",
   "metadata": {},
   "source": [
    "# 3. What is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b5c2d",
   "metadata": {},
   "source": [
    "Gradient clipping involves forcing the gradient values (element-wise) to a specific minimum or maximum value if the gradient exceeded an expected range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed71a67",
   "metadata": {},
   "source": [
    "# 4. Explain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177610d8",
   "metadata": {},
   "source": [
    "A neural network is considered to be an effort to mimic human brain actions in a simplified manner. Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d3579",
   "metadata": {},
   "source": [
    "# 5. Explain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e596dec",
   "metadata": {},
   "source": [
    "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a9560",
   "metadata": {},
   "source": [
    "# 6. Explain self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac19ed0",
   "metadata": {},
   "source": [
    "In layman's terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd895459",
   "metadata": {},
   "source": [
    "# 7. What is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721433e5",
   "metadata": {},
   "source": [
    "Bahdanau Attention is also known as Additive attention as it performs a linear combination of encoder states and the decoder states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b36c40",
   "metadata": {},
   "source": [
    "# 8. What is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a15ab",
   "metadata": {},
   "source": [
    "A language model is basically a probability distribution over words or word sequences. In practice, a language model gives the probability of a certain word sequence being “valid”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1811db0",
   "metadata": {},
   "source": [
    "# 9. What is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964ff65",
   "metadata": {},
   "source": [
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afbdca",
   "metadata": {},
   "source": [
    "# 10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac4033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
